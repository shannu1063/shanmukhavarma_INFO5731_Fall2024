{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shannu1063/shanmukhavarma_INFO5731_Fall2024/blob/main/Kothapalli_ShanmukhaVarma_Exercise_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Finding the most popular books across genres on an online bookstore\n",
        "\n",
        "Data needed:\n",
        "\n",
        "Title of the book\n",
        "Price of the book\n",
        "Ratings\n",
        "Availability\n",
        "Category or genre\n",
        "\n",
        "Amount of data needed:\n",
        "\n",
        "1000 books from multiple genres\n",
        "\n",
        "Steps on Collecting the data and Saving it:\n",
        "\n",
        "Choosing Data source: Bookstore I chose: https://books.toscrape.com/\n",
        "Information collection: Getting books information like title, price and ratings\n",
        "Implementation: Implementing collection of data through python\n",
        "Saving Data: Storing the data in csv.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "zgw3UTtDFmrr",
        "outputId": "0cf11089-146a-4ec5-dcaa-8262586cc64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFinding the most popular books across genres on an online bookstore\\n\\nData needed:\\n\\nTitle of the book\\nPrice of the book\\nRatings\\nAvailability\\nCategory or genre\\n\\nAmount of data needed:\\n\\n1000 books from multiple genres\\n\\nSteps on Collecting the data and Saving it:\\n\\nChoosing Data source: Bookstore I chose: https://books.toscrape.com/\\nInformation collection: Getting books information like title, price and ratings\\nImplementation: Implementing collection of data through python\\nSaving Data: Storing the data in csv.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "BASE_URL_ID = \"http://books.toscrape.com/\"\n",
        "\n",
        "def getting_page_content(url):\n",
        "    response_received = requests.get(url)\n",
        "    if response_received.status_code == 200:\n",
        "        return response_received.content\n",
        "    else:\n",
        "        print(f\"Error while receiving the content\")\n",
        "        return None\n",
        "\n",
        "def scraping_books_page(url):\n",
        "    books_array = []\n",
        "    content = getting_page_content(url)\n",
        "    if content is None:\n",
        "        return books_array\n",
        "\n",
        "    soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "    for book in soup.find_all('article', class_='product_pod'):\n",
        "        title = book.h3.a['title']\n",
        "        price = book.find('p', class_='price_color').text.strip()\n",
        "        availability = book.find('p', class_='instock availability').text.strip()\n",
        "        rating = book.p['class'][1]\n",
        "\n",
        "        #This is the data we are trying to collect\n",
        "        books_array.append({\n",
        "            'Title': title,\n",
        "            'Price': price,\n",
        "            'Rating': rating,\n",
        "            'Availability': availability\n",
        "        })\n",
        "\n",
        "    return books_array\n",
        "\n",
        "def scraping_all_books():\n",
        "    books_data_array = []\n",
        "\n",
        "    next_url = BASE_URL_ID + \"catalogue/page-1.html\"\n",
        "\n",
        "    while next_url:\n",
        "        print(f\"Scraping this page: {next_url}\")\n",
        "        books = scraping_books_page(next_url)\n",
        "        books_data_array.extend(books)\n",
        "\n",
        "        available_content = getting_page_content(next_url)\n",
        "        if available_content is None:\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(available_content, 'html.parser')\n",
        "        next_page = soup.find('li', class_='next')\n",
        "\n",
        "        if next_page:\n",
        "            next_url = BASE_URL_ID + \"catalogue/\" + next_page.a['href']\n",
        "        else:\n",
        "            next_url = None\n",
        "    return books_data_array\n",
        "\n",
        "# Function to save data\n",
        "def save_books_to_csv(books_data, filename='books_data.csv'):\n",
        "    df = pd.DataFrame(books_data)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Saving data to {filename}\")\n",
        "\n",
        "#Using single function\n",
        "def collect_books_data():\n",
        "    books_data = scraping_all_books()\n",
        "    save_books_to_csv(books_data)\n",
        "\n",
        "#calling the function\n",
        "collect_books_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thmLFDeJTuZU",
        "outputId": "c67cf516-6477-43c8-b7ba-dfdd699aab08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping this page: http://books.toscrape.com/catalogue/page-1.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-2.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-3.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-4.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-5.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-6.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-7.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-8.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-9.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-10.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-11.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-12.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-13.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-14.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-15.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-16.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-17.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-18.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-19.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-20.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-21.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-22.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-23.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-24.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-25.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-26.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-27.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-28.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-29.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-30.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-31.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-32.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-33.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-34.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-35.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-36.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-37.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-38.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-39.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-40.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-41.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-42.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-43.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-44.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-45.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-46.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-47.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-48.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-49.html\n",
            "Scraping this page: http://books.toscrape.com/catalogue/page-50.html\n",
            "Saving data to books_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c785b65-56c8-4b0c-f3c6-977165fdca82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles on 'machine learning' published from 2014 to 2024...\n",
            "Articles collected and saved to the file machine learning_articles.csv\n",
            "\n",
            "1. Predicting IVF live birth probabilities using machine learning, center-specific and national registry-based models\n",
            "   Authors: ET Nguyen, MG Retzloff, LA Gago, JE Nichols…\n",
            "   Year: Not Applicable\n",
            "   Venue: Not Applicable\n",
            "\n",
            "2. Deep Learning in Leaf Disease Detection (2014–2024): A Visualization-Based Bibliometric Analysis\n",
            "   Authors: J Chaki, D Ghosh\n",
            "   Year: Not Applicable\n",
            "   Venue: Not Applicable\n",
            "\n",
            "3. An analysis of Urban Sprawl Growth and Prediction using Remote Sensing and Machine learning techniques for Vila Velha, Brazil\n",
            "   Authors: A Al Mazroa, M Maashi, F Kouki, KM Othman…\n",
            "   Year: Not Applicable\n",
            "   Venue: Not Applicable\n",
            "\n",
            "4. [HTML][HTML] Machine Learning-Based Retrieval of Total Ozone Column Amount and Cloud Optical Depth from Irradiance Measurements\n",
            "   Authors: M Sztipanov, L Krizsán, W Li, JJ Stamnes, T Svendby…\n",
            "   Year: Not Applicable\n",
            "   Venue: Not Applicable\n",
            "\n",
            "5. Smart Home Technology Research Landscape: A Bibliometric Analysis of Scientific Publications (2014–2024)\n",
            "   Authors: M Ibrahim, MA Mahmoud, MA Al\n",
            "   Year: Not Applicable\n",
            "   Venue: Not Applicable\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "\n",
        "def scrape_google_scholar(keyword, start_year, end_year, num_of_articles=1000):\n",
        "    base_url = \"https://scholar.google.com/scholar\"\n",
        "    articles_array = []\n",
        "    start = 0 #indexing\n",
        "\n",
        "#setting up headers\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    while len(articles_array) < num_of_articles:\n",
        "        params = {\n",
        "            'q': f'{keyword} {start_year}-{end_year}',\n",
        "            'hl': 'en',\n",
        "            'start': start,\n",
        "            'as_ylo': start_year,\n",
        "            'as_yhi': end_year\n",
        "        }\n",
        "\n",
        "        response = requests.get(base_url, params=params, headers=headers)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for result in soup.select('.gs_r.gs_or.gs_scl'):\n",
        "            title_elem = result.select_one('.gs_rt')\n",
        "            authors_elem = result.select_one('.gs_a')\n",
        "            abstract_elem = result.select_one('.gs_rs')\n",
        "\n",
        "            if title_elem and authors_elem:\n",
        "                title = title_elem.text.strip()\n",
        "                authors_info = authors_elem.text.strip()\n",
        "                abstract = abstract_elem.text.strip() if abstract_elem else \"N/A\"\n",
        "\n",
        "                year = \"Not Applicable\"\n",
        "                venue = \"Not Applicable\"\n",
        "                info_parts = authors_info.split('-')\n",
        "                if len(info_parts) > 1:\n",
        "                    year_venue = info_parts[-1].strip()\n",
        "                    year_parts = year_venue.split(',')\n",
        "                    if len(year_parts) > 1:\n",
        "                        year = year_parts[-1].strip()\n",
        "                        venue = year_parts[0].strip()\n",
        "                    elif year_venue.isdigit():\n",
        "                        year = year_venue\n",
        "\n",
        "                articles_array.append({\n",
        "                    'Title': title,\n",
        "                    'Authors': authors_info.split('-')[0].strip(),\n",
        "                    'Year': year,\n",
        "                    'Venue': venue,\n",
        "                    'Abstract': abstract\n",
        "                })\n",
        "\n",
        "                if len(articles_array) >= num_of_articles:\n",
        "                    break\n",
        "\n",
        "        if len(articles_array) >= num_of_articles:\n",
        "            break\n",
        "\n",
        "        start += 10 #going to the next page in the results\n",
        "        time.sleep(random.uniform(10, 20))\n",
        "    return articles_array[:num_of_articles]\n",
        "\n",
        "def save_to_csv(articles, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['Title', 'Authors', 'Year', 'Venue', 'Abstract']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for article in articles:\n",
        "            writer.writerow(article)\n",
        "\n",
        "def main():\n",
        "    keyword = \"machine learning\" #used keyword machine learning instead of XYZ\n",
        "    start_year = 2014\n",
        "    end_year = 2024\n",
        "    num_of_articles = 10\n",
        "\n",
        "    print(f\"Articles on '{keyword}' published from {start_year} to {end_year}...\")\n",
        "    articles_array = scrape_google_scholar(keyword, start_year, end_year, num_of_articles)\n",
        "\n",
        "    if articles_array:\n",
        "        output_file = f\"{keyword}_articles.csv\"\n",
        "\n",
        "    #Function for Saving the articles\n",
        "        save_to_csv(articles_array, output_file)\n",
        "        print(f\"Articles collected and saved to the file {output_file}\")\n",
        "\n",
        "#printing the first five articles to check the working of the code\n",
        "        for i, article in enumerate(articles_array[:5], 1):\n",
        "            print(f\"\\n{i}. {article['Title']}\")\n",
        "            print(f\"   Authors: {article['Authors']}\")\n",
        "            print(f\"   Year: {article['Year']}\")\n",
        "            print(f\"   Venue: {article['Venue']}\")\n",
        "    else:\n",
        "        print(\"Articles not collected.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ebb2db-6379-439a-bfdc-9c19a151fc25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "post collected: Canada imposes a 100% tariff o...\n",
            "post collected: Disabled man drags himself off...\n",
            "post collected: Rainbow Bridge between US, Can...\n",
            "post collected: Air Canada customers kicked of...\n",
            "post collected: A population of hard-to-eradic...\n",
            "post collected: 'Now 15 per cent is rude': Tip...\n",
            "post collected: One suspect found dead, one st...\n",
            "post collected: Philadelphia is under a 'code ...\n",
            "post collected: Thousands told to evacuate due...\n",
            "post collected: 'I thought it was a joke': Can...\n",
            "post collected: ‘Zero tolerance’: Police move ...\n",
            "post collected: Norad monitoring high-altitude...\n",
            "post collected: Canada: Multiple killed in sho...\n",
            "post collected: Smoke will keep pouring into t...\n",
            "post collected: Proud Boys Canada Dissolves...\n",
            "post collected: Canada's vaccination rate over...\n",
            "post collected: Proud Boys added to Canada’s l...\n",
            "post collected: Facebook reports a decline in ...\n",
            "post collected: Trudeau announces Canada is ba...\n",
            "post collected: Canada: At least 10 people kil...\n",
            "post collected: Canada police arrive to remove...\n",
            "post collected: US judge cancels permit for Ke...\n",
            "post collected: Ryan Reynolds asks young peopl...\n",
            "post collected: American couple charged for no...\n",
            "post collected: Transport Canada says if you c...\n",
            "post collected: The Pope went to Canada to apo...\n",
            "post collected: Canada’s last fully intact Arc...\n",
            "post collected: Man planning to commit terror ...\n",
            "post collected: Canada pulls out of 2020 Tokyo...\n",
            "post collected: High-altitude object shot down...\n",
            "post collected: Fires intensify in Canada, cou...\n",
            "post collected: Canada Border Services seizes ...\n",
            "post collected: Queen Victoria and Elizabeth I...\n",
            "post collected: Florida can import prescriptio...\n",
            "post collected: 2 dead after vehicle crashes i...\n",
            "post collected: Canada teen wins $1000 a week ...\n",
            "post collected: U.S. hits EU, Canada and Mexic...\n",
            "post collected: Terrorism ruling first for Can...\n",
            "post collected: Canada set to become largest c...\n",
            "post collected: Americans driving to Canada fo...\n",
            "post collected: Air Canada’s $10 million in bo...\n",
            "post collected: Canada bans cellphone unlockin...\n",
            "post collected: Canada hits back at U.S. with ...\n",
            "post collected: 'Appalling': Woman bumped from...\n",
            "post collected: Canada oil pipeline spills 200...\n",
            "post collected: Canada will ensure border offi...\n",
            "post collected: Sen. Rand Paul scheduled to ha...\n",
            "post collected: Anonymous donor gives $100M to...\n",
            "post collected: U.S. no longer \"Safe Third Cou...\n",
            "post collected: China threatens Canada with 'g...\n",
            "post collected: Canada says it has info linkin...\n",
            "post collected: The Latest: US plan to allow p...\n",
            "post collected: TransCanada Keystone pipeline ...\n",
            "post collected: ‘An indescribable moment’: Ind...\n",
            "post collected: Canada-U.S. border closure ext...\n",
            "post collected: Bus full of seniors heading to...\n",
            "post collected: Jim Estill, the CEO of an appl...\n",
            "post collected: Canada: outcry after video sho...\n",
            "post collected: Saudi Arabia halts all medical...\n",
            "post collected: Canada, top exporter of steel ...\n",
            "post collected: Google Searches for \"How to Mo...\n",
            "post collected: 9,300 employees locked out: La...\n",
            "post collected: Remains of Madison Scott found...\n",
            "post collected: Whale and dolphin captivity ba...\n",
            "post collected: Canada lawmakers vote to legal...\n",
            "post collected: Miss World Canada prevented fr...\n",
            "post collected: Chinese warship nearly hits U....\n",
            "post collected: Runaway Kangaroo punches polic...\n",
            "post collected: ‘You’re in Canada now’: B.C. m...\n",
            "post collected: Air Canada ordered to pay cust...\n",
            "post collected: Nearly 5,000 U.S. citizens tri...\n",
            "post collected: Greta Thunberg mural in Edmont...\n",
            "post collected: Whistleblower warns baffling i...\n",
            "post collected: Canada's surging cost of livin...\n",
            "post collected: Canada vows to be next country...\n",
            "post collected: Canada advises LGBTQ2 travelle...\n",
            "post collected: Protesters block Canada-bound ...\n",
            "post collected: Canada to accept 20,000 vulner...\n",
            "post collected: Air Canada passenger wakes up ...\n",
            "post collected: More than 3,400 Americans reje...\n",
            "post collected: Canada's Bianca Andreescu defe...\n",
            "post collected: Canada to create Airline overb...\n",
            "post collected: Leaked sales scheme by Kia Can...\n",
            "post collected: People vow to boycott Ben and ...\n",
            "post collected: Canada to require all commerci...\n",
            "post collected: Report: Nearly 3 billion fewer...\n",
            "post collected: Canada demands U.S. end ‘right...\n",
            "post collected: U.S., Canada hail Taiwan's 'fr...\n",
            "post collected: Not just Oregon: Saudi student...\n",
            "post collected: Canada reports no new deaths f...\n",
            "post collected: Exclusive: Drugmakers offer Ca...\n",
            "post collected: Ice caps formed during Little ...\n",
            "post collected: 'Primed to burn:' Former Parks...\n",
            "post collected: Canada cracking down on Americ...\n",
            "post collected: More churches burn down on Can...\n",
            "post collected: Vancouver BC is now cut off to...\n",
            "post collected: Canada’s attorney general has ...\n",
            "post collected: Poison mailed to White House m...\n",
            "post collected: Canada deploys military aircra...\n",
            "post collected: Air Canada makes changes after...\n",
            "Collected posts and saved them to reddit_data.csv\n"
          ]
        }
      ],
      "source": [
        "#Here I am trying to collect data from reddit of the posts regarding the country Canada\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "import random\n",
        "\n",
        "def collect_reddit_data(subreddit, keyword, num_of_posts=100):\n",
        "#using new reddit url as base url\n",
        "    base_url = f\"https://www.reddit.com/r/{subreddit}/search.rss\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    params = {\n",
        "        'q': keyword,\n",
        "        'restrict_sr': 'on',\n",
        "        'sort': 'relevance',\n",
        "        'limit': 100 #here we are trying to limit the posts to 100 so that it consumes less time to implement\n",
        "    }\n",
        "\n",
        "    data_array = []\n",
        "    while len(data_array) < num_of_posts:\n",
        "        try:\n",
        "            response = requests.get(base_url, headers=headers, params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            root = ET.fromstring(response.content)\n",
        "\n",
        "            #I am trying to use the feed which is accessible publicly beacuse reddit is blocking web scrapping methods\n",
        "            for entry in root.findall('.//{http://www.w3.org/2005/Atom}entry'):\n",
        "                title = entry.find('{http://www.w3.org/2005/Atom}title').text\n",
        "                author = entry.find('{http://www.w3.org/2005/Atom}author/{http://www.w3.org/2005/Atom}name').text\n",
        "                link = entry.find('{http://www.w3.org/2005/Atom}link').attrib['href']\n",
        "                published = entry.find('{http://www.w3.org/2005/Atom}published').text\n",
        "                updated = entry.find('{http://www.w3.org/2005/Atom}updated').text\n",
        "\n",
        "                content = entry.find('{http://www.w3.org/2005/Atom}content').text\n",
        "                domain = link.split('/')[2] if link.startswith('http') else 'self.'+subreddit\n",
        "\n",
        "                data_array.append({\n",
        "                    'title': title,\n",
        "                    'author': author,\n",
        "                    'url_link': link,\n",
        "                    'published_info': published,\n",
        "                    'updated_info': updated,\n",
        "                    'domain_info': domain,\n",
        "                    'content_info': content[:1000] if content else ''\n",
        "                })\n",
        "\n",
        "                print(f\"post collected: {title[:30]}...\")\n",
        "\n",
        "                #loop breaks when criteria is satisfied\n",
        "                if len(data_array) >= num_of_posts:\n",
        "                    break\n",
        "\n",
        "            if len(root.findall('.//{http://www.w3.org/2005/Atom}entry')) < params['limit']:\n",
        "                break\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"error while retrieving the data: {e}\")\n",
        "            break\n",
        "\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    return pd.DataFrame(data_array[:num_of_posts])\n",
        "\n",
        "# here i am trying to print how many news posts are posted relating to the country canada\n",
        "subreddit = \"news\"\n",
        "keyword = \"canada\"\n",
        "df = collect_reddit_data(subreddit, keyword)\n",
        "\n",
        "#saving the data to csv\n",
        "df.to_csv('reddit_data.csv', index=False)\n",
        "print(f\"Collected posts and saved them to reddit_data.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Learning Experience: Got to learn about data collection and how to work on different tools to collect data and extracting relevant information and examining it.\n",
        "\n",
        "Challenges Encountered: Working with large datasets and making sure data is accurate.\n",
        "\n",
        "Relevance to Your Field of Study: It helps in analysing ratings from users and gives in-depth understanding which is helpful commerce-related fields.\n",
        "'''"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "55W9AMdXCSpV"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}